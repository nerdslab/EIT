<!doctype html>
<html>

<head>
    <title>EIT: Embedded Interaction Transformer</title>
    <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
    <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
    <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
    <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="js/menu.js"></script>
    <style>
        .menu-index {
            color: rgb(255, 255, 255) !important;
            opacity: 1 !important;
            font-weight: 700 !important;
        }
    </style>
</head>

<!--
<style>
    img{
        max-width: 100%;
        max-height: 100%;
        display: block; /* remove extra space below image */
    }
    .box{
        width: 500px;
    }
    .box.large{
        height: auto;
    }
</style>
-->

<body>
<div class="menu-container"></div>
<div class="content-container">
    <div class="content">
        <div class="content-table flex-column">

            <!--Start Intro-->
            <div class="flex-row">
                <div class="flex-item flex-column">
                    <h2 class="add-top-margin">Welcome to EIT! (NeurIPS 2022)</h2>
                    <p class="text add-top-margin">
                        This webpage is still in progress. The information may not be accurate.
                        <br>
                        The paper preprint can be found <a target="https://arxiv.org/pdf/2206.06131.pdf" href="https://arxiv.org/pdf/2206.06131.pdf">here</a>.
                        If you use our code, please cite our paper as below.
                        <br>
                        <br>
                        [<b>cite</b>] Ran Liu, Mehdi Azabou, Max Dabagia, Jingyun Xiao, and Eva L. Dyer. 
                        "Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers." 
                        Advances in Neural Information Processing Systems 35 (2022).
                    </p>
                    
                    <h2 class="add-top-margin">Abstract</h2>
                    <hr>
                    <p class="text add-top-margin">
                        Complex systems (such as the brain) contain multiple individual elements (e.g. neurons) that interact
                        dynamically to generate their outputs. The process by which these local interactions give rise to
                        large-scale behaviors is important in many domains of science and engineering, from ecology and social networks
                        to microbial interactions and brain dynamics.
                        A natural way to model the activity of a system is to build a collective or <em>population-level view</em>,
                        where we consider individuals (or channels) jointly to determine the dynamics of the population. 
                        However, studying systems from this population-level perspective might cause the model to lose sight of the 
                        contributions of different individuals’ dynamics to the final prediction or inference. 
                        Moving forward, we need methods that can build good population-level representations while also providing 
                        an interpretable view of the data at the individual level.
                    </p>

                    <h2 class="add-top-margin">Methods</h2>
                    <hr>
                    <p class="text add-top-margin">
                        EIT (Embedded Interaction Transformer) presents a new framework for modeling time-varying observations 
                        of a system by using dynamic embeddings of individual channels to construct a population-level view.
                    </p>
                    <p class="text">
                        EIT decomposes multi-channel time-series by first learning rich features from individual time-series before 
                        incorporating information and learned interactions across different individuals in the population.
                        One critical benefit of our model is <b>spatial/individual separability</b>: it builds a population-level 
                        representation from embeddings of individual channels, which naturally leads to channel-level permutation invariance. 
                        In domain generalization tasks, this means a trained model can be tested with permuted channels or entirely
                        different number of channels.
                    </p>
                </div>
            </div>
            <div class="flex-row">
                <div class="flex-item flex-column">
                    <a class="image center adaptive-image" style="background-image: url('img/overview.png'); width: 100%; max-width: 1200px; min-height: 400px;" href="img/overview.png" target="_blank"></a>
                    <p class="text text-center graph-title">
                        The overview image of EIT.
                    </p>
                    <p class="image-caption">
                        <b>(A)</b> A traditional state-space view would treat the collective dynamics as a population right from the beginning,
                        and use a population encoder to learn how the dynamics progress along time, which creates a highly abstracted latent space. 
                        <b>(B)</b> EIT learns individual dynamics with an individual encoder at the beginning. After establishing individual representation,
                        we feed them into an interaction encoder to build a population representation. The two encoders work together to build a 
                        representation space that is richer than that of the traditional method. <b>(C)</b> The detailed architecture: EIT consists of an 
                        individual transformer that processes data for each individual, an interaction transformer that processes embeddings at each 
                        timepoint, and two projection modules at the end of both transformers.
                    </p>
                    
                    <h2 class="add-top-margin">Contributions</h2>
                    <hr>
                    <p class="text add-top-margin">
                        Our contributions are as follows:
                        <ul>
                            <li>We proposed a novel framework that considers individual dynamics before collective dynamics, and realized 
                                it with a multi-stage transformer <b>EIT</b>. EIT learns both population representation and individual representation 
                                through decoupling the dynamics of individuals and their interactions in multi-variate time-series. </li>
                            <li>We introduced methods for generalization across datasets of different input size (number of channels) and ordering. 
                                We further proposed to measure the alignments of time-series in different datasets through the Wasserstein divergence. </li>
                            <li>We applied EIT to both many-body systems and neural systems recordings. After demonstrating our model’s robust 
                                decoding performance, we validated its ability to transfer individual dynamics by performing domain generalization 
                                across different populations of neurons and finding neuron alignments. </li>
                        </ul>
                    </p>

                </div>
            </div>
            <!--End Intro-->
            
            

            <!--Start Text Only-->
            <div class="flex-row">
                <div class="flex-item flex-column">
                    <h2 class="add-top-margin">Team</h2>
                    <hr>
                    <ol class="publication">
                        <li>
                            <p class="text-small-margin">
                                Ran Liu (<a target="_blank" href="https://ranliu98.github.io/">web</a>),
                                Mehdi Azabou,
                                Max Dabagia (<a target="_blank" href="https://mdabagia.github.io/">web</a>),
                                Jingyun Xiao, 
                                Eva Dyer (<a target="_blank" href="https://dyerlab.gatech.edu/people/pi-profile/">web</a>).
                            </p>
                        </li>
                    </ol>
                </div>
            </div>
            <!--End Text Only-->

        </div>
    </div>
</div>
</body>

</html>
